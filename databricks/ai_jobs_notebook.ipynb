{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2334778-f610-4742-80db-0237f06f8b94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, BooleanType\n",
    "from pyspark.sql.functions import from_json, col, regexp_extract, when, regexp_replace, concat, lit\n",
    "from datetime import datetime\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.storage.blob import BlobServiceClient, ContainerClient, BlobPrefix\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# variables\n",
    "container_name = 'aijobs'\n",
    "storage_name = 'storageacctszymon'\n",
    "source_folder = \"source_files\" # folder where source files are\n",
    "sas_token = \"0000000\" #TODO generate SAS token in your Azure Storage\n",
    "\n",
    "# spark conf\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "spark.conf.set(\"fs.azure.account.key.storageacctszymon.blob.core.windows.net\", sas_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe67e7f-8a75-48ff-8ab0-8dc7e3271ba8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_dir = f\"wasbs://{container_name}@{storage_name}.blob.core.windows.net/source_files/\"\n",
    "\n",
    "file_list = dbutils.fs.ls(source_dir)\n",
    "\n",
    "csv_files = []\n",
    "\n",
    "for file in file_list:\n",
    "    if 'processed' not in str(file): #or 'class' not in file or 'school' not in file:\n",
    "        csv_files.append(file)\n",
    "\n",
    "if len(csv_files) != 1:\n",
    "    raise SystemExit(\"Multiple files found. Aborting job.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052aea70-3877-44d8-9e1c-8f0a7e2fcdef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of rows before processing: 1534\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([ \n",
    "StructField('id', \n",
    "            IntegerType(), True), \n",
    "StructField('title', \n",
    "            StringType(), True), \n",
    "StructField('senior', \n",
    "            BooleanType(), True), \n",
    "StructField('company', \n",
    "            StringType(), False), \n",
    "StructField('location', \n",
    "            StringType(), False) , \n",
    "StructField('salary_range', \n",
    "            StringType(), False), \n",
    "StructField('currency', \n",
    "            StringType(), True)\n",
    "]) \n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\",True).schema(schema).load(f\"wasbs://{container_name}@{storage_name}.blob.core.windows.net/source_files/*.csv\")\n",
    "\n",
    "raw_df_count = df.count()\n",
    "\n",
    "print(f\"No. of rows before processing: {raw_df_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df434df6-0eed-431f-a1e5-ef306cdc4088",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "\n",
    "df = df.withColumn(\"salary_range\", regexp_replace(col(\"salary_range\"), \"\\u00A0\", \" \"))\n",
    "\n",
    "# Define the regular expression to match the salary components\n",
    "pattern_min = r\"(\\d{1,3}\\s\\d{3})\"\n",
    "pattern_max = r\"â€“\\s*(\\d{1,3}\\s\\d{3})\"\n",
    "\n",
    "\n",
    "# Extract the first and second parts of the salary and concatenate them\n",
    "df = df.withColumns(\n",
    "    {\n",
    "        \"salary_min\": \n",
    "        when(\n",
    "            col(\"salary_range\").rlike(pattern_min),  # Check if the salary pattern exists\n",
    "            concat(regexp_extract(col(\"salary_range\"), pattern_min, 1))\n",
    "        ).otherwise(\"N/A\"),\n",
    "        \"salary_max\": \n",
    "        when(\n",
    "            col(\"salary_range\").rlike(pattern_max),  # Check if the salary pattern exists\n",
    "            concat(regexp_extract(col(\"salary_range\"), pattern_max, 1))\n",
    "        ).otherwise(\"N/A\")\n",
    "    }\n",
    ")\n",
    "\n",
    "# df.show(truncate=False)\n",
    "\n",
    "df = df.withColumns(\n",
    "    {\n",
    "        \"salary_max\": regexp_replace('salary_max', ' ', ''),\n",
    "        \"salary_min\": regexp_replace('salary_min', ' ', '')\n",
    "    })\n",
    "\n",
    "# df.show()\n",
    "\n",
    "pattern_plus = r\"\\+\"\n",
    "\n",
    "df = df.withColumn('location',\n",
    "        when(\n",
    "            col(\"location\").rlike(pattern_plus), \n",
    "            'Multiple'\n",
    "        ).otherwise(col('location')))\n",
    "\n",
    "# df.show(truncate=False)\n",
    "\n",
    "df = df.drop('salary_range') \\\n",
    "\n",
    "df = df.withColumn('date', lit(datetime.today().strftime('%Y-%m-%d')))\n",
    "\n",
    "df = df.select(\"id\", \"date\", \"title\",\"senior\",\"company\", \"location\", \"salary_min\", \"salary_max\", \"currency\")\n",
    "\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebff3b93-4ae2-4a30-8869-505e47c93f64",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "datetime_now = datetime.now().strftime('%Y-%m-%dT-%H-%M-%S')\n",
    "path = f\"wasbs://{container_name}@{storage_name}.blob.core.windows.net/logs/{datetime_now}\"  # protocols: wasbs, abfs (blob.core), abfss (dfs.core)\n",
    "\n",
    "(df\n",
    " .coalesce(1)  # Ensure only one part file is created\n",
    " .write\n",
    " .mode(\"overwrite\")\n",
    " .option(\"header\", \"true\")\n",
    " .csv(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d7e62e7-e39c-4676-857a-bda3c5003133",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file wasbs://aijobs@storageacctszymon.blob.core.windows.net/logs/2024-08-23T-12-15-55/part-00000-tid-5076326388662699174-de2f9b32-e504-4eaa-81b7-d99ac5e2a1aa-6-1-c000.csv moved to wasbs://aijobs@storageacctszymon.blob.core.windows.net/transformed_files/ai_jobs_transformed_2024-08-23T-12-15-55.csv\n"
     ]
    }
   ],
   "source": [
    "# List the files in the directory\n",
    "files = dbutils.fs.ls(path)\n",
    "\n",
    "# Search for the file with the part-00000 prefix\n",
    "part_file = next((f.path for f in files if f.name.startswith(\"part-00000\")), None)\n",
    "\n",
    "# Ensure a matching file was found\n",
    "if part_file:\n",
    "    # Define the destination path\n",
    "    destination = f\"wasbs://{container_name}@{storage_name}.blob.core.windows.net/transformed_files/ai_jobs_transformed_{datetime_now}.csv\"\n",
    "    \n",
    "    # Move the file to the desired destination with the new name\n",
    "    dbutils.fs.mv(part_file, destination)\n",
    "    print(f\"Processed file {part_file} moved to {destination}\")\n",
    "else:\n",
    "    print(\"No file with prefix 'part-00000' found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "372a3023-8283-4867-90b7-e461f797136e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file: source_files/jobs_combined.csv\n",
      "Processing Completed. File 'source_files/jobs_combined.csv' moved to 'source_files/raw_processed/jobs_combined.csv'. Job completed.\n"
     ]
    }
   ],
   "source": [
    "# Construct the BlobServiceClient using the SAS token\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{storage_name}.blob.core.windows.net\", credential=sas_token)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# List all CSV files in the source folder\n",
    "blobs = container_client.list_blobs(name_starts_with=source_folder + '/')\n",
    "\n",
    "csv_files = []\n",
    "\n",
    "# Print and validate blob names\n",
    "for blob in blobs:\n",
    "    if 'processed' not in str(blob.name):\n",
    "        csv_files.append(blob.name)\n",
    "\n",
    "# Get the only CSV file\n",
    "csv_file = csv_files[0]\n",
    "\n",
    "# Print source file path for debugging  \n",
    "print(f\"Source file: {csv_file}\")\n",
    "\n",
    "processed_df_count = df.count()\n",
    "\n",
    "x_percent = 50\n",
    "\n",
    "if (processed_df_count / raw_df_count)*100 < x_percent:\n",
    "    print(f'Reconciliation error: Over {100 - x_percent} percent of rows got removed in processing. ({raw_df_count - processed_df_count} out of {raw_df_count}) Moving file to not_processed folder & aborting job')\n",
    "    # Destination path in the subfolder \n",
    "    destination_blob_name = f\"{source_folder}/raw_not_processed/{csv_file.split('/')[-1]}\"\n",
    "\n",
    "    # Copy the file to the destination\n",
    "    source_blob_client = container_client.get_blob_client(blob=csv_file)\n",
    "    destination_blob_client = container_client.get_blob_client(blob=destination_blob_name)\n",
    "\n",
    "    # Start the copy operation\n",
    "    destination_blob_client.start_copy_from_url(source_blob_client.url)\n",
    "\n",
    "    # After the copy completes, delete the source file (optional)\n",
    "    container_client.delete_blob(csv_file)\n",
    "    raise SystemExit(f\"File Not Processed. File '{csv_file}' moved to '{destination_blob_name}'. Job aborted.\")\n",
    "else:\n",
    "    # Destination path in the subfolder\n",
    "    destination_blob_name = f\"{source_folder}/raw_processed/{csv_file.split('/')[-1]}\"\n",
    "\n",
    "    # Copy the file to the destination\n",
    "    source_blob_client = container_client.get_blob_client(blob=csv_file)\n",
    "    destination_blob_client = container_client.get_blob_client(blob=destination_blob_name)\n",
    "\n",
    "    # Start the copy operation\n",
    "    destination_blob_client.start_copy_from_url(source_blob_client.url)\n",
    "\n",
    "    # After the copy completes, delete the source file (optional)\n",
    "    container_client.delete_blob(csv_file)\n",
    "\n",
    "    print(f\"Processing Completed. File '{csv_file}' moved to '{destination_blob_name}'. Job completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ai_jobs_notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
